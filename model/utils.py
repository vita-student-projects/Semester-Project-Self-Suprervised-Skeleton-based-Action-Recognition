import torch
import math
import torch.nn as nn
from collections import OrderedDict
import matplotlib.pyplot as plt
import numpy as np
import os
from torch.distributed import init_process_group, destroy_process_group
import torch.distributed as dist
import json

class EMA():
   def __init__(self, alpha):
       super().__init__()
       self.alpha = alpha

   def update_ema(self, teacher, student):
       if teacher is None:
           return student
       return teacher * self.alpha + (1 - self.alpha) * student


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


USE_GPU = True
if USE_GPU and torch.cuda.is_available():

    def to_device(x, gpu=None):
        x = x.cuda(gpu)
        return x

else:

    def to_device(x, gpu=None):
        return x


if USE_GPU and torch.cuda.is_available():

    def to_var(x, requires_grad=False, gpu=None):
        x = x.cuda(gpu)
        return x.requires_grad_(requires_grad)

else:

    def to_var(x, requires_grad=False, gpu=None):
        return x.requires_grad_(requires_grad)


class Metrics:

    def __init__(self, *keys):
        self.keys = keys
        self.metrics = self.reset_dict()

    def reset_dict(self):
        ret = OrderedDict()
        for key in self.keys:
            ret[key] = []
        return ret

    def update(self, key, val):
        self.metrics[key].append(val)

    def ordered_update(self, *vals):
        for key, val in zip(self.keys, vals):
            self.update(key, val)

    def ordered_mean(self):
        ret = []
        for key in self.metrics:
            ret.append(np.mean(self.metrics[key]))
        return ret

    def log_latest(self, header="", verbose=True):
        ret = header
        for k, v in self.metrics.items():
            ret += f"\t[{k}] {v[-1]:.4f}"
        if verbose:
            print(ret)
        return ret

    def find_best(self, key, lower_is_better=True):
        if len(self.metrics[key]) == 0:
            return (np.inf, {}) if lower_is_better else (-np.inf, {})
        best_index, best = min(
            enumerate(self.metrics[key]), key=lambda t: t[1] if lower_is_better else -t[1])
        best_dict = dict(best_index=best_index)
        for key in self.keys:
            best_dict[key] = self.metrics[key][best_index]
        return best, best_dict

    def save(self, path):
        write_json(self.metrics, path)

    @classmethod
    def load(cls, path):
        with open(path) as f:
            parsed = json.loads(f.read())
        ret = cls(*parsed.keys())
        for key in ret.keys:
            ret.metrics[key] = parsed[key]
        return ret

    def plot(self, key, marker='-'):
        plt.plot(self.metrics[key], marker)


def write_json(ctx, path, verbose=False):
    with open(path, "w") as f:
        json_string = json.dumps(ctx, indent=4)
        f.write(json_string)
    if verbose:
        print(json_string)


def ddp_setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355"

    init_process_group(backend="nccl", rank=rank, world_size=world_size)


def reduce_mean(tensor, nprocs):  # 用于平均所有gpu上的运行结果，比如loss
    rt = tensor.clone()
    dist.all_reduce(rt, op=dist.ReduceOp.SUM)
    rt /= nprocs
    return rt


class L2Loss(nn.Module):
    def __init__(self):
        super(L2Loss, self).__init__()

    def forward(self, pred, target):
        return torch.mean(torch.square(pred - target))